visNodes(size = nodes_df$value, font = list(size = 45, color = "#444444", face = "georgia", bold = TRUE)) %>%
visEdges(smooth = FALSE) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(keyboard = TRUE, dragNodes = TRUE, dragView = TRUE, zoomView = TRUE) %>%
visLayout(randomSeed = 42) %>%
visEvents(
type = "once",
startStabilizing = "function() { this.moveTo({scale:0.15}); }")
author_net
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
library(textstem)
#library(SnowballC)
setwd("~/Desktop/Spring 2025/Data Viz/R Directory - Data_Viz/Sanchez_Sevastian_DataViz/11_arxiv_GRADED")
arxiv <- read_csv("data/arxiv_subset.csv")
# Chunk 3
# Tokenizing and cleaning abstracts
abstract_words <- arxiv %>%
select(id, abstract, update_date) %>%
unnest_tokens(word, abstract) %>% # Splits into words
filter(!str_detect(word, "\\d")) %>% # Removes numbers
anti_join(get_stopwords()) %>% # Removes stopwords
filter(!word %in% c("’", "“", "”", "–")) %>%  # Removes punctuation attached to words
mutate(word = lemmatize_words(word)) #lemmatizes words (matches words to root word)
# Identifying top 20 terms across all abstracts (2018-2023)
top_terms_all <- abstract_words %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
# Plot:  top 20 terms across all abstracts (all years)
ggplot(top_terms_all, aes(x = reorder(word, n), y = n, fill = n)) + #descending order
geom_col() +
scale_fill_gradient(low = "#6878ad", high = "#3D5296FF") +
coord_flip() +
labs(x = NULL, y = 'Counts',
title = 'Top 20 Most Frequent Words',
subtitle = 'Analyzing Abstracts from ArXiv Journal Publications (2018-2023)') +
theme_minimal() + theme(legend.position="none")
# Chunk 4
# Filters to words from 2019 and 2023 only
abstract_words_19_23 <- abstract_words %>%
mutate(year = year(update_date)) %>% # extracts only year in 'update_date' column
filter(year %in% c(2019, 2023)) # filters for only 2019 and 2023
#DF term frequency: counts words from these years
term_freq_year <- abstract_words_19_23 %>%
count(year, word, sort = TRUE)
# Vector: gets top 10 terms specifically aggregated from 2019 and 2023
top_terms_19_23 <- term_freq_year %>%
group_by(word) %>%
summarise(total = sum(n)) %>%
slice_max(total, n = 10) %>% #top 10 words
pull(word)
# Filter to these top terms
comparison_data <- term_freq_year %>%
filter(word %in% top_terms_19_23) %>%
complete(year, word, fill = list(n = 0))   # Ensure all terms appear in both years
#Plot: top 10 words
ggplot(comparison_data, aes(x = reorder(word, n), y = n, fill = factor(year))) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("2019" = "#3D5296FF", "2023" = "#36A9ABFF")) +
coord_flip() +
labs(x = NULL, y = 'Counts',
title = 'Top 10 Most Frequent Words (2019 vs. 2023)',
subtitle = 'Analyzing Abstracts from ArXiv Journal Publications') +
theme_minimal()
# Chunk 5
#setting up
top_100 <- abstract_words %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 100)
#plotting
wordcloud(top_100$word, top_100$n,
scale = c(3, 0.5), colors = viridis::viridis(100))
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(igraph)
library(visNetwork)
setwd("~/Desktop/Spring 2025/Data Viz/R Directory - Data_Viz/Sanchez_Sevastian_DataViz/11_arxiv_GRADED")
arxiv <- read_csv("data/arxiv_subset.csv")
# Chunk 3
#wrangling
filtered <- arxiv %>%
filter(str_detect(categories, "physics.soc-ph") & str_detect(categories, "cs.SI")) %>%
separate_rows(authors, sep = ",") %>%
separate_rows(authors, sep = " and ") %>%
mutate(authors = str_trim(authors)) %>%
filter(authors != "")
# Recognizing accent marks
filtered <- filtered %>%
mutate(
# Acute accents (á, é, etc.)
authors = str_replace_all(authors, fixed("\\'a"), "á"),
authors = str_replace_all(authors, fixed("\\'e"), "é"),
authors = str_replace_all(authors, fixed("\\'i"), "í"),
authors = str_replace_all(authors, fixed("\\'o"), "ó"),
authors = str_replace_all(authors, fixed("\\'u"), "ú"),
# Grave accents (à, è, etc.)
authors = str_replace_all(authors, fixed("\\`a"), "à"),
authors = str_replace_all(authors, fixed("\\`e"), "è"),
authors = str_replace_all(authors, fixed("\\`i"), "ì"),
authors = str_replace_all(authors, fixed("\\`o"), "ò"),
authors = str_replace_all(authors, fixed("\\`u"), "ù"),
# Umlaut/diaeresis (ä, ö, etc.)
authors = str_replace_all(authors, fixed('\\"a'), "ä"),
authors = str_replace_all(authors, fixed('\\"o'), "ö"),
authors = str_replace_all(authors, fixed('\\"u'), "ü"),
# Circumflex (â, ê, etc.)
authors = str_replace_all(authors, fixed("\\^a"), "â"),
authors = str_replace_all(authors, fixed("\\^e"), "ê"),
authors = str_replace_all(authors, fixed("\\^i"), "î"),
authors = str_replace_all(authors, fixed("\\^o"), "ô"),
authors = str_replace_all(authors, fixed("\\^u"), "û"),
# Tilde (ñ, Ñ)
authors = str_replace_all(authors, fixed("\\~n"), "ñ"),
authors = str_replace_all(authors, fixed("\\~N"), "Ñ"),
# Cedilla (ç, Ç)
authors = str_replace_all(authors, fixed("\\c{c}"), "ç"),
authors = str_replace_all(authors, fixed("\\c{C}"), "Ç"),
# Remove any remaining backslashes
authors = str_replace_all(authors, fixed("\\"), "")
)
#paper_counts DF
paper_counts_df <- filtered %>%
count(authors, name = "paper_count")
#converting var to numeric in paper_counts DF
paper_counts_df$paper_count <- as.numeric(paper_counts_df$paper_count)
#merging paper_counts to filtered
filtered <- filtered %>%
left_join(paper_counts_df, by = 'authors')
# further filtering to authors w/ more than 4 papers
prolific_authors <- filtered %>%
filter(paper_count >= 4)
# creating vector
prolific_authors_vect <- prolific_authors %>%
pull(authors)
#edges
edges <- filtered %>%
group_by(id) %>%
mutate(prolific_in_paper = authors %in% prolific_authors_vect) %>%
filter(prolific_in_paper) %>%
filter(n() >= 2) %>%
summarise(authors = list(unique(authors)), .groups = "drop") %>%
mutate(pairs = purrr::map(authors, ~ combn(.x, 2, simplify = FALSE))) %>%
unnest(pairs) %>%
mutate(
from = map_chr(pairs, ~ .x[1]),
to = map_chr(pairs, ~ .x[2])
) %>%
select(from, to)
#adding weights based on paper number
edges_weighted <- edges %>%
count(from, to, name = "weight")
#output
g <- graph_from_data_frame(edges_weighted, directed = FALSE)
#checking NAs for papers
V(g)$papers <- paper_counts_df$paper_count[match(V(g)$name, paper_counts_df$authors)]
if (any(is.na(V(g)$papers))) {
V(g)$papers[is.na(V(g)$papers)] <- 0
}
# Chunk 4
# --- Centrality Calculation ---
V(g)$betweenness <- betweenness(g, normalized = TRUE)
# --- Top Authors for Labeling ---
top_authors <- V(g)$name[order(V(g)$papers, decreasing = TRUE)][1:15]
# --- Nodes and Edges for visNetwork ---
nodes_df <- data.frame(
id = V(g)$name,
label = ifelse(V(g)$name %in% top_authors, V(g)$name, ""),
value = V(g)$papers,
color = colorRampPalette(c("#54C568FF", "#36A9ABFF", "#3487A6FF", "#3D5296FF"))(100)[
cut(V(g)$betweenness, breaks = 400, labels = FALSE)
],
title = paste0(
"<b>", V(g)$name, "</b><br>",
"Papers: ", V(g)$papers, "<br>",
"Betweenness: ", round(V(g)$betweenness, 3)
)
)
edges_df <- edges_weighted %>%
select(from, to)
# --- Interactive Network Visualization ---
author_net <- visNetwork(nodes_df, edges_df, height = "600px", width = "100%") %>%
visIgraphLayout(layout = "layout_nicely") %>%
visNodes(size = nodes_df$value, font = list(size = 45, color = "#444444", face = "georgia", bold = TRUE)) %>%
visEdges(smooth = FALSE) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(keyboard = TRUE, dragNodes = TRUE, dragView = TRUE, zoomView = TRUE) %>%
visLayout(randomSeed = 42) %>%
visEvents(
type = "once",
startStabilizing = "function() { this.moveTo({scale:0.15}); }")
author_net
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(igraph)
library(visNetwork)
setwd("~/Desktop/Spring 2025/Data Viz/R Directory - Data_Viz/Sanchez_Sevastian_DataViz/11_arxiv_GRADED")
arxiv <- read_csv("data/arxiv_subset.csv")
# Chunk 3
#wrangling
filtered <- arxiv %>%
filter(str_detect(categories, "physics.soc-ph") & str_detect(categories, "cs.SI")) %>%
separate_rows(authors, sep = ",") %>%
separate_rows(authors, sep = " and ") %>%
mutate(authors = str_trim(authors)) %>%
filter(authors != "")
# Recognizing accent marks
filtered <- filtered %>%
mutate(
# Acute accents (á, é, etc.)
authors = str_replace_all(authors, fixed("\\'a"), "á"),
authors = str_replace_all(authors, fixed("\\'e"), "é"),
authors = str_replace_all(authors, fixed("\\'i"), "í"),
authors = str_replace_all(authors, fixed("\\'o"), "ó"),
authors = str_replace_all(authors, fixed("\\'u"), "ú"),
# Grave accents (à, è, etc.)
authors = str_replace_all(authors, fixed("\\`a"), "à"),
authors = str_replace_all(authors, fixed("\\`e"), "è"),
authors = str_replace_all(authors, fixed("\\`i"), "ì"),
authors = str_replace_all(authors, fixed("\\`o"), "ò"),
authors = str_replace_all(authors, fixed("\\`u"), "ù"),
# Umlaut/diaeresis (ä, ö, etc.)
authors = str_replace_all(authors, fixed('\\"a'), "ä"),
authors = str_replace_all(authors, fixed('\\"o'), "ö"),
authors = str_replace_all(authors, fixed('\\"u'), "ü"),
# Circumflex (â, ê, etc.)
authors = str_replace_all(authors, fixed("\\^a"), "â"),
authors = str_replace_all(authors, fixed("\\^e"), "ê"),
authors = str_replace_all(authors, fixed("\\^i"), "î"),
authors = str_replace_all(authors, fixed("\\^o"), "ô"),
authors = str_replace_all(authors, fixed("\\^u"), "û"),
# Tilde (ñ, Ñ)
authors = str_replace_all(authors, fixed("\\~n"), "ñ"),
authors = str_replace_all(authors, fixed("\\~N"), "Ñ"),
# Cedilla (ç, Ç)
authors = str_replace_all(authors, fixed("\\c{c}"), "ç"),
authors = str_replace_all(authors, fixed("\\c{C}"), "Ç"),
# Remove any remaining backslashes
authors = str_replace_all(authors, fixed("\\"), "")
)
#paper_counts DF
paper_counts_df <- filtered %>%
count(authors, name = "paper_count")
#converting var to numeric in paper_counts DF
paper_counts_df$paper_count <- as.numeric(paper_counts_df$paper_count)
#merging paper_counts to filtered
filtered <- filtered %>%
left_join(paper_counts_df, by = 'authors')
# further filtering to authors w/ more than 4 papers
prolific_authors <- filtered %>%
filter(paper_count >= 4)
# creating vector
prolific_authors_vect <- prolific_authors %>%
pull(authors)
#edges
edges <- filtered %>%
group_by(id) %>%
mutate(prolific_in_paper = authors %in% prolific_authors_vect) %>%
filter(prolific_in_paper) %>%
filter(n() >= 2) %>%
summarise(authors = list(unique(authors)), .groups = "drop") %>%
mutate(pairs = purrr::map(authors, ~ combn(.x, 2, simplify = FALSE))) %>%
unnest(pairs) %>%
mutate(
from = map_chr(pairs, ~ .x[1]),
to = map_chr(pairs, ~ .x[2])
) %>%
select(from, to)
#adding weights based on paper number
edges_weighted <- edges %>%
count(from, to, name = "weight")
#output
g <- graph_from_data_frame(edges_weighted, directed = FALSE)
#checking NAs for papers
V(g)$papers <- paper_counts_df$paper_count[match(V(g)$name, paper_counts_df$authors)]
if (any(is.na(V(g)$papers))) {
V(g)$papers[is.na(V(g)$papers)] <- 0
}
# Chunk 4
# Centrality Calculation
V(g)$betweenness <- betweenness(g, normalized = TRUE)
# Top Authors for Labeling
top_authors <- V(g)$name[order(V(g)$papers, decreasing = TRUE)][1:15]
# Nodes and Edges used for visNetwork package
nodes_df <- data.frame(
id = V(g)$name,
label = ifelse(V(g)$name %in% top_authors, V(g)$name, ""),
value = V(g)$papers,
color = colorRampPalette(c("#54C568FF", "#36A9ABFF", "#3487A6FF", "#3D5296FF"))(100)[
cut(V(g)$betweenness, breaks = 400, labels = FALSE)
],
title = paste0(
"<b>", V(g)$name, "</b><br>",
"Papers: ", V(g)$papers, "<br>",
"Betweenness: ", round(V(g)$betweenness, 3)
)
)
#df for edges
edges_df <- edges_weighted %>%
select(from, to)
# Chunk 5
# Interactive Network Visualization
author_net <- visNetwork(nodes_df, edges_df, height = "600px", width = "100%") %>%
visIgraphLayout(layout = "layout_nicely") %>%
visNodes(size = nodes_df$value, font = list(size = 45, color = "#444444", face = "georgia", bold = TRUE)) %>%
visEdges(smooth = FALSE) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(keyboard = TRUE, dragNodes = TRUE, dragView = TRUE, zoomView = TRUE) %>%
visLayout(randomSeed = 42) %>%
visEvents(
type = "once",
startStabilizing = "function() { this.moveTo({scale:0.15}); }")
author_net
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(igraph)
library(visNetwork)
setwd("~/Desktop/Spring 2025/Data Viz/R Directory - Data_Viz/Sanchez_Sevastian_DataViz/11_arxiv_GRADED")
arxiv <- read_csv("data/arxiv_subset.csv")
# Chunk 3
#wrangling
filtered <- arxiv %>%
filter(str_detect(categories, "physics.soc-ph") & str_detect(categories, "cs.SI")) %>%
separate_rows(authors, sep = ",") %>%
separate_rows(authors, sep = " and ") %>%
mutate(authors = str_trim(authors)) %>%
filter(authors != "")
# Recognizing accent marks
filtered <- filtered %>%
mutate(
# Acute accents (á, é, etc.)
authors = str_replace_all(authors, fixed("\\'a"), "á"),
authors = str_replace_all(authors, fixed("\\'e"), "é"),
authors = str_replace_all(authors, fixed("\\'i"), "í"),
authors = str_replace_all(authors, fixed("\\'o"), "ó"),
authors = str_replace_all(authors, fixed("\\'u"), "ú"),
# Grave accents (à, è, etc.)
authors = str_replace_all(authors, fixed("\\`a"), "à"),
authors = str_replace_all(authors, fixed("\\`e"), "è"),
authors = str_replace_all(authors, fixed("\\`i"), "ì"),
authors = str_replace_all(authors, fixed("\\`o"), "ò"),
authors = str_replace_all(authors, fixed("\\`u"), "ù"),
# Umlaut/diaeresis (ä, ö, etc.)
authors = str_replace_all(authors, fixed('\\"a'), "ä"),
authors = str_replace_all(authors, fixed('\\"o'), "ö"),
authors = str_replace_all(authors, fixed('\\"u'), "ü"),
# Circumflex (â, ê, etc.)
authors = str_replace_all(authors, fixed("\\^a"), "â"),
authors = str_replace_all(authors, fixed("\\^e"), "ê"),
authors = str_replace_all(authors, fixed("\\^i"), "î"),
authors = str_replace_all(authors, fixed("\\^o"), "ô"),
authors = str_replace_all(authors, fixed("\\^u"), "û"),
# Tilde (ñ, Ñ)
authors = str_replace_all(authors, fixed("\\~n"), "ñ"),
authors = str_replace_all(authors, fixed("\\~N"), "Ñ"),
# Cedilla (ç, Ç)
authors = str_replace_all(authors, fixed("\\c{c}"), "ç"),
authors = str_replace_all(authors, fixed("\\c{C}"), "Ç"),
# Remove any remaining backslashes
authors = str_replace_all(authors, fixed("\\"), "")
)
#paper_counts DF
paper_counts_df <- filtered %>%
count(authors, name = "paper_count")
#converting var to numeric in paper_counts DF
paper_counts_df$paper_count <- as.numeric(paper_counts_df$paper_count)
#merging paper_counts to filtered
filtered <- filtered %>%
left_join(paper_counts_df, by = 'authors')
# further filtering to authors w/ more than 4 papers
prolific_authors <- filtered %>%
filter(paper_count >= 4)
# creating vector
prolific_authors_vect <- prolific_authors %>%
pull(authors)
#edges
edges <- filtered %>%
group_by(id) %>%
mutate(prolific_in_paper = authors %in% prolific_authors_vect) %>%
filter(prolific_in_paper) %>%
filter(n() >= 2) %>%
summarise(authors = list(unique(authors)), .groups = "drop") %>%
mutate(pairs = purrr::map(authors, ~ combn(.x, 2, simplify = FALSE))) %>%
unnest(pairs) %>%
mutate(
from = map_chr(pairs, ~ .x[1]),
to = map_chr(pairs, ~ .x[2])
) %>%
select(from, to)
#adding weights based on paper number
edges_weighted <- edges %>%
count(from, to, name = "weight")
#output
g <- graph_from_data_frame(edges_weighted, directed = FALSE)
#checking NAs for papers
V(g)$papers <- paper_counts_df$paper_count[match(V(g)$name, paper_counts_df$authors)]
if (any(is.na(V(g)$papers))) {
V(g)$papers[is.na(V(g)$papers)] <- 0
}
# Chunk 4
# Centrality Calculation
V(g)$betweenness <- betweenness(g, normalized = TRUE)
# Top Authors for Labeling
top_authors <- V(g)$name[order(V(g)$papers, decreasing = TRUE)][1:15]
# Nodes and Edges used for visNetwork package
nodes_df <- data.frame(
id = V(g)$name,
label = ifelse(V(g)$name %in% top_authors, V(g)$name, ""),
value = V(g)$papers,
color = colorRampPalette(c("#54C568FF", "#36A9ABFF", "#3487A6FF", "#3D5296FF"))(100)[
cut(V(g)$betweenness, breaks = 400, labels = FALSE)
],
title = paste0(
"<b>", V(g)$name, "</b><br>",
"Papers: ", V(g)$papers, "<br>",
"Betweenness: ", round(V(g)$betweenness, 3)
)
)
#df for edges
edges_df <- edges_weighted %>%
select(from, to)
# Chunk 5
# Interactive Network Visualization
author_net <- visNetwork(nodes_df, edges_df, height = "600px", width = "100%") %>%
visIgraphLayout(layout = "layout_nicely") %>%
visNodes(size = nodes_df$value, font = list(size = 45, color = "#444444", face = "georgia", bold = TRUE)) %>%
visEdges(smooth = FALSE) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(keyboard = TRUE, dragNodes = TRUE, dragView = TRUE, zoomView = TRUE) %>%
visLayout(randomSeed = 42) %>%
visEvents(
type = "once",
startStabilizing = "function() { this.moveTo({scale:0.15}); }")
author_net
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud)
library(textstem)
#library(SnowballC)
setwd("~/Desktop/Spring 2025/Data Viz/R Directory - Data_Viz/Sanchez_Sevastian_DataViz/11_arxiv_GRADED")
arxiv <- read_csv("data/arxiv_subset.csv")
# Chunk 3
# Tokenizing and cleaning abstracts
abstract_words <- arxiv %>%
select(id, abstract, update_date) %>%
unnest_tokens(word, abstract) %>% # Splits into words
filter(!str_detect(word, "\\d")) %>% # Removes numbers
anti_join(get_stopwords()) %>% # Removes stopwords
filter(!word %in% c("’", "“", "”", "–")) %>%  # Removes punctuation attached to words
mutate(word = lemmatize_words(word)) #lemmatizes words (matches words to root word)
# Identifying top 20 terms across all abstracts (2018-2023)
top_terms_all <- abstract_words %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 20)
# Plot:  top 20 terms across all abstracts (all years)
ggplot(top_terms_all, aes(x = reorder(word, n), y = n, fill = n)) + #descending order
geom_col() +
scale_fill_gradient(low = "#6878ad", high = "#3D5296FF") +
coord_flip() +
labs(x = NULL, y = 'Counts',
title = 'Top 20 Most Frequent Words',
subtitle = 'Analyzing Abstracts from ArXiv Journal Publications (2018-2023)') +
theme_minimal() + theme(legend.position="none")
# Chunk 4
# Filters to words from 2019 and 2023 only
abstract_words_19_23 <- abstract_words %>%
mutate(year = year(update_date)) %>% # extracts only year in 'update_date' column
filter(year %in% c(2019, 2023)) # filters for only 2019 and 2023
#DF term frequency: counts words from these years
term_freq_year <- abstract_words_19_23 %>%
count(year, word, sort = TRUE)
# Vector: gets top 10 terms specifically aggregated from 2019 and 2023
top_terms_19_23 <- term_freq_year %>%
group_by(word) %>%
summarise(total = sum(n)) %>%
slice_max(total, n = 10) %>% #top 10 words
pull(word)
# Filter to these top terms
comparison_data <- term_freq_year %>%
filter(word %in% top_terms_19_23) %>%
complete(year, word, fill = list(n = 0))   # Ensure all terms appear in both years
#Plot: top 10 words
ggplot(comparison_data, aes(x = reorder(word, n), y = n, fill = factor(year))) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("2019" = "#3D5296FF", "2023" = "#36A9ABFF")) +
coord_flip() +
labs(x = NULL, y = 'Counts',
title = 'Top 10 Most Frequent Words (2019 vs. 2023)',
subtitle = 'Analyzing Abstracts from ArXiv Journal Publications') +
theme_minimal()
# Chunk 5
#setting up
top_100 <- abstract_words %>%
count(word, sort = TRUE) %>%
slice_max(n, n = 100)
#plotting
wordcloud(top_100$word, top_100$n,
scale = c(3, 0.5), colors = viridis::viridis(100))
